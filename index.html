<!DOCTYPE HTML>

<html>
	<head>
		<title>Portfolio Artur Skrzeta</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><a href="https://arturskrzeta.github.io/"><img src="images/avatar.png" alt="" style="margin-bottom:0px;"/></a></span>
					<h1 id="logo"><a href="https://arturskrzeta.github.io/">Artur Skrzeta</a></h1>
					<p>Data Analyst<br />
					+5 years experience</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Intro</a></li>
						<li><a href="#two">Features</a></li>
						<li><a href="#three">Demo</a></li>
						<li><a href="#four">Setup</a></li>
						<li><a href="#five">Source Code</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/artur-skrz%C4%99ta-010b23187/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.instagram.com/arturskrrr/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="https://github.com/ArturSkrzeta" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto: arturskrzeta@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">
										<h2>Data Warehousing for Business Insights</h2>
										<h3>Intro</h3>
										<p>This section presents a process of data collection, processing and analyzing to provide meaningful business insights.
											I also provide some theoretical introduction on what tools and techniques need to be applied to make it works.</p>

										<h5>Data Integration:</h5>
										<ul>
											<li>Combining data from different sources and providing users with unified view of them.</li>
											<li>It integrates data from heterogeneous database systems and transforms it into a single coherent data store.</li>
											<li>We can find it helpful in data mining and business analysis.</li>
										</ul>

										<h5>ETL:</h5>
										<ul>
											<li>It can be defined as <b><i>a repeatable programmed data movement</i></b>.</li>
											<li>It stands for:
												<br>
												- <b>extract</b>: getting data from different sources.<br>
												- <b>transform</b>: filter/map/enrich/combine/validate/aggregate/sort/applying business rules to the data<br>
												- <b>load</b>: store data in the data warehouse or data mart.
											</li>
											<li>Data Warehouse (DW) vs Data Mart (DM):
												<br>
												- DW is broadly focused on all the departments. It is possible that it can even represent the entire company.<br>
												- DM is a specific business subject-oriented, and it is used at a department level.
											</li>
											<li>ETL process can be applied in making data strucutred from data lake keeping unstructrued data from different sources:
												<br><br>
												<img src="images/data_lake_before_dw.JPG">
											</li>
										</ul>

										<h5>ETL designing:</h5>
										<ol>
											<li>Operational db:
												<br>
												- we don't go to an operaional database directly as we don't want to weigh its performance down with analytical functions,<br>
												- operaitonal db is only resposible for transactions saving which cosumes a lot of performance power.
											</li>
											<li>Flat files (CSV, JSON, XML):
												<br>
												- extracting data from operational db like a snapsot with additional meatadata like timestamp, system name etc.<br>
												- extracting flat files and working with them ensures any operaional db perfomrance issue risk.
											</li>
											<li>Raw database:
												<br>
												- loading data as it is in the flat files without any business logic applied yet,<br>
												- the purpose is to be able to check if the data makes sense, for instance if the user name is ok, or if the zip-code has an expected format, if the commas is in a wrong place etc, <br>
												- we perform some QA check on the fields matching, if some data don't fall into inappropriate db field,<br>
												- here, we don't apply any transofrmation logic as we want to test pure data, otherwise it would be hard to debug some issues if they come from either the logic or data itslef,<br>
												- we can move on with some logging here to track what files has been loaded, how many rows etc.
											</li>
											<li>Staging database:
												<br>
												- here we apply business logic, some mappings, tandarization, deduplicaiton, denormalization to transform the data up to business requirements,<br>
												- QA perfomed after transomration applied - we can be chekcing if the amount of distinct records is still as expected after applying some aggregating funcitons,<br>
												- basically, QA at this stage confirms if I can trust the data or not,<br>
												- we can also here do some logging on how many records have been loaded, what stred procedures were run, if they were successfull or not, how long they work.
											</li>
											<li>Data Warehouse:
												<br>
												- final destination of ETL,<br>
												- here we proceed with further data analysis,<br>
												- can be a back-end of Tableau dashboard,<br>
												- consists of fact tables and dimension tables.
											</li>
											<li>Here is the schema that shows the pahses of ETL:
												<br>
												<img src="images/etl_3_phases.JPG">
												<span style="font-size:14px;">source: Seattle Data Guy</span>
											</li>
										</ol>

										<h5>Additonal things for ETL:</h5>
										<ul>
											<li>Data QA,</li>
											<li>Error hanlding and logging - tracking when etl fails, tracking why is that - it's important for the maintenance,</li>
											<li>Dashborad for logs visualization,</li>
											<li>Data linage and data catalog - tracking fileds dependencies,</li>
											<li>Metadata database - tracking pipelines dependencies.</li>
										</ul>

										<h5>QA on database:</h5>
										<ul>
											<li>We may want to refer to previous data.</li>
											<li>We can perform many checks simply with SELECT queries.</li>
											<li>We can perform following checks:
												<br>
												- Categorical checks - when we know a field has a very specific value.<br>
												- Null checks - depending if the fileds accept ones.<br>
												- Aggregate checks - counting records both in raw table and prod table and comparing the results.
													It may appear thet some records in prod are missing and we can thind the mission one in the raw table.
													These kind of changes can be a result of accidental removal throughout the process.<br>
												- Anomaly checks - test for data that doesn't make sense - f.e. average values is lower than minimum value.<br>
											</li>
											<li>QA for db is not as such developed and standardized as for app's code.
													Most ofthen, in order to get app tested out, the testing tools and environments peel away dependency layers such as databases.
													This means a traditional QA unit testing practices are designed to overlook the databases just to get focused on examination of the code itselft.
											</li>
										</ul>

										<h5>Data Lake:</h5>
											<ul>
												<li>Can store different forms of data:
													<br>
													- structured data from relational databases (rows and columns),<br>
													- semi-structured data (CSV, logs, XML, JSON),<br>
													- unstructured data (emails, documents, PDFs),<br>
													- binary data (images, audio, video).
												 </li>
												 <li>Can be both established on-premise or in the cloud.</li>
												 <li>There is also a data swamp which is unmanagable and inaccessible or low-value-providing data lake.</li>
											</ul>

										<h5>ELT:</h5>
										<ul>
											<li>It stands for Extract, Load and Transform.</li>
											<li>This process first loads the data into the target storage and then leverages the target system to do the transformation.</li>
											<li>It makes sense when target system is based on high-performace engine like Hadoop cluster.</li>
											<li>Hadoop clusters work by breaking a problem into smaller chunks, then distributing those chunks across a large number of machines for processing.</li>
										</ul>

										<h5>Data Ingestion:</h5>
										<ul>
											<li>It's the process of obtaining and importing data for immediate use or long-term storage in a database.</li>
											<li>To ingest something means literally to take something in or absorb something. Data can be streamed in real time or ingested in batches.</li>
										</ul>

										<h5>Db vs DW</h5>
										<ul>
											<li>Db:
												<br>
												- mainly for transactional databases,<br>
												- many inserts and updates or deletes,<br>
												- for live apps,<br>
												- normalized,<br>
												- saves space,<br>
												- limits possible performacnce and redundancy issues,<br>
												- mainly for operational purposes.
											</li>
											<li>Dw:
												<br>
												- centralizes and standardizes all of the data from either company's or third party's applicatons,<br>
												- for analytical purposes separating it from transactional systems in order to keep the performance,<br>
												- consists of fact tables that groups all transactions,<br>
												- consists of dimension tables with which we can break down transactions for example by category/regiona and sum up the sales,<br>
												- used by analytical teams to quickly select the data without joining tables and understanding whole relationship logic.
											</li>
										</ul>

										<h5>Fact Table and Dimension Table:</h5>
										<ul>
											<li>Fact table is a central table of the <b>star schema</b> in a data warehouse. It stores quantitative information for the analysis.</li>
											<li>Fact table being kept denormalized in order to combine multiple tables into one so that it can be queried quickly.</li>
											<li>A fact is every event that can undergo analysis.
													Facts have measures, for instance a transaction (fact) may have 2 measures (value, units sold) - numerical or boolean.
											</li>
											<li>A fact table works with dimension tables.
													 	While the fact table holds the data to be analyzed, the dimension table stores data about the ways in which the data in the fact table can be analyzed.
											</li>
											<li>Here is how we can visualize the dependecy between facts and dimensions:
<!-- start -->
<pre>
SELECT salary
FROM employees
WHERE country IN ('Poland', 'Germany', 'France')
</pre>
<!-- stop -->
												- <code>salary</code> is one of facts and <code>country</code> is one of the dimensions.
											</li>
											<li>Thus, the fact table consists of two types of columns:
												<br>
												- foreign key columns that joins dimension tables (Time ID,	Product ID,	Customer ID for example below),<br>
												- measure columns containing quantitative data being analyzed (Unit Sold).
											</li>
											<li>Here is the example of fact table. Every sale is a fact that happens, and the fact table is used to record these facts:
												<br>
												<span style="font-size:12px;">source: searchdatamanagement.techtarget.com</span>
												<table>
												 <tbody>
												  <tr>
												   <td>Time ID</td>
												   <td>Product ID</td>
												   <td>Customer ID</td>
												   <td>Unit Sold</td>
												  </tr>
												  <tr>
												   <td>4</td>
												   <td>17</td>
												   <td>2</td>
												   <td>1</td>
												  </tr>
												  <tr>
												   <td>8</td>
												   <td>21</td>
												   <td>3</td>
												   <td>2</td>
												  </tr>
												  <tr>
												   <td>8</td>
												   <td>4</td>
												   <td>1</td>
												   <td>1</td>
												  </tr>
												 </tbody>
												</table>
											</li>
											<li>Dimension table:
												<br>
												<span style="font-size:12px;">source: searchdatamanagement.techtarget.com</span>
												<table>
												 <tbody>
												  <tr>
												   <td>Customer ID</td>
												   <td>Name</td>
												   <td>Gender</td>
												   <td>Income</td>
												   <td>Education</td>
												   <td>Region</td>
												  </tr>
												  <tr>
												   <td>1</td>
												   <td>Brian Edge</td>
												   <td>M</td>
												   <td>2</td>
												   <td>3</td>
												   <td>4</td>
												  </tr>
												  <tr>
												   <td>2</td>
												   <td>Fred Smith</td>
												   <td>M</td>
												   <td>3</td>
												   <td>5</td>
												   <td>1</td>
												  </tr>
												  <tr>
												   <td>3</td>
												   <td>Sally Jones</td>
												   <td>F</td>
												   <td>1</td>
												   <td>7</td>
												   <td>3</td>
												  </tr>
												 </tbody>
												</table>
											</li>
											<li>In the above example, the customer ID column, in the fact table, is the foreign key that joins with the dimension table. Let's have a look at fact table's 2nd row:
												<br>
												-  row 2 of the fact table records the fact that customer 3, Sally Jones, bought two items on day 8.<br>
											</li>
											<li>We can put table structure through:
												<br><br>
												<table>
													<thead>
														<tr>
															<th>Normalization</th>
															<th>Denormaliaztion</th>
														</tr>
													</thead>
													<tbody>
														<tr>
															<td>Used to remove redundant data.</td>
															<td>Used to combine multiple tables for quick querying.</td>
														</tr>
														<tr>
															<td>Focuses on clearing the database from unused data.</td>
															<td>Focuses on achieving the faster execution.</td>
														</tr>
														<tr>
															<td>Uses optimized memory.</td>
															<td>Causes some sort of wastage of memory.</td>
														</tr>
														<tr>
															<td>Used where number of insert/update/delete operations are performed and joins of those tables are not expensive.</td>
															<td>Used where joins are expensive and frequent query is executed on the tables.</td>
														</tr>
														<tr>
															<td>Ensures data integrity i.e. any addition or deletion of data from the table will not create any mismatch in the relationship of the tables.</td>
															<td>Doesn't ensure data integrity.</td>
														</tr>
													</tbody>
												</table>
											</li>
											<li>DWH structure where we have <u>denormalized</u> fact table (or a few of them) containing measures and foreign keys to <u>denormalized</u> dimension tables is called <b>star schema</b>.</li>
											<li>DWH structure where we have <u>denormalized</u> fact table (or a few of them) containing measures and foreign keys to <u>normalized</u> dimension is called <b>snowflake schema</b>.</li>
										</ul>

										<h5>Slowly Changing Dimensions:</h5>
										<ul>
											<li>They are dimensions for which attributes change <b>slowly over time</b> due to business needs.</li>
											<li>Slowly over time means changes happen occasionally rather than on a regular schedule.</li>
											<li>We can apply different approaches:
													<br>
													- type 1: overwriting the value with the new one and not tracking the historical data;<br>
													- type 2: preserving the history by creating another records with updated value and unique primary key - we can put anothe column with versioning;<br>
													- type 3: preserving the partial history where we create a pair of attributes to track current value and previous one;<br>
											</li>
										</ul>

										<h5>Columnar databae:</h5>
										<ul>
											<li>In relational databases, tables are defined as an unordered collection of rows.</li>
											<li>However, columnar database stores data in columnar sequence instead of row's.</li>
											<li>Here is employees table:
<!-- start -->
<pre>
rowid		NrPrac		Imie	 	Nazwisko	Pensja
001 		10 		Alicja 		Zielinska	5200
002 		11 		Bogdan 		Wrona		4900
003 		12		Czesław 	Kowalski 	5600
004 		114		Dominik 	Wrona 		5200
</pre>
<!-- stop -->
												<span style="font-size:12px;">source: if.uj.edu.pl</span><br>
												- in columnar store, above data looks like following:
<!-- start -->
<pre>
10:001; 11:002; 12:003; 114:004;
Alicja:001; Bogdan:002; Czesław:003; Dominik:004;
Zielinska:001; Wrona:002,004; Kowalski:003;
5200:001,004; 4900:002; 5600:003;
</pre>
<!-- stop -->
												<span style="font-size:12px;">source: if.uj.edu.pl</span><br>
												- where the first line contains the values of the whole column named <code>NrPrac</code><br>
												- the same for the rest of columns,<br>
												- attribute's value indicates specific <code>rowid</code>.<br>
											</li>
											<li>When quering database about one or tow columns then a relational database's engine needs to read all records anyway.
													It slows down the overall performance.
													In columnar database, engine read only columns that we query.
													We fetch data fast and only that data we need.
													On the other hand, any record operations are very slow in a columnar database.
											</li>
											<li>Becasue fact tables in data warehouses are often long (many rows) and wide (many columns) and their data doesn't change while OLAP, they are being stored in te columnar form.</li>
										</ul>

										<h5>Data Warehous (DWH) desing:</h5>
										<ul>
											<li>Designing DWH we need to pay closer attention to business requirements of further reporting.</li>
											<li>Data dimensions and granuality of data needs to be adjusted to reports that will be issued based on the DWH.</li>
											<li>The granuality depends on business requirements.
													In sales, besides price and units sold, we can associate transaction fact with foreign keys to dimensions as time, product, shop, salesman and even more.
													The more dimensions, the more foreign keys, the more detailed analysis in the OLAP.</li>
											<li>Dimensions need to follow granuality, no the other way around.</li>
										</ul>

										<h5>DWH working modes:</h5>
										<ul>
											<li>Operational or analytical mode:
												<br>
												- operations limited only to analytical queries,<br>
												- no for any data or table schemas modifications,<br>
												-	data in DWH stays unchanged.
											</li>
											<li>Modification mode:
												<br>
												- authorized users ingest the data into DWH,<br>
												- it can be done automatically on specific time,<br>
												- there can be also actualization of indexes perfomed,<br>
												- analytical users loses connection at time of DWH modificaiton,<br>
												- any schamas modification may pose a danger for any predefiend queries and report.
											</li>
										</ul>

										<h5>Query optimalization:</h5>
										<ul>
											<li>DWH is espacially optimized for reading operations.</li>
											<li>To keep high level of optimalizaton we should be avoiding performing one operation many times but we want to redefine query to run it only once instead.</li>
											<li>Here is unefficeint way to query with WHERE clause:
<!-- start -->
<pre>
SELECT ...
FROM ...
WHERE X AND P1

SELECT ...
FROM ...
WHERE X AND P2

........................

SELECT ...
FROM ...
WHERE X AND Pn
</pre>
<!-- end -->
												- as we can see X condition repeateas itself many times in WHERE clause,<br>
												- we can take condition X as general condition put on the data,<br>
												- applying that X condition reduces data set into a smaller sub-set,<br>
												- instead of putting X condtion with every select we can put it once in the <b>materialized view</b>:<br>
<!-- start -->
<pre>
CREATE MATERIALIZED VIEW my_mat_view AS
SELECT ...
FROM ...
WHERE X
</pre>
<!-- end -->
												- we obtain subset resulting from filtering data set into sub-set that is in line with X condition,<br>
												- we can then proceed with further variable conditions: P1, P2, Pn:
<!-- start -->
<pre>
SELECT ...
FROM my_mat_view
WHERE P1

SELECT ...
FROM my_mat_view
WHERE P2

........................

SELECT ...
FROM my_mat_view
WHERE Pn
</pre>
<!-- end -->
												- the main difference between a materialized view and a regular view:
												<table>
													<thead>
														<tr>
															<td>Materialized view</td>
															<td>Regular view</td>
														</tr>
													</thead>
													<tbody>
														<tr>
															<td>When calling, it references to outcome is physically saved in the system.</td>
															<td>When calling, it performs hidden query execution.</td>
														</tr>
														<tr>
															<td>It's getting refreshed automtically.</td>
															<td>There is no automated refreshing.</td>
														</tr>
													</tbody>
												</table>
											</li>
										</ul>

									</header>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h3>Features</h3>
									<p>App includes following features:</p>
									<ul class="feature-icons">
										<li class="icon solid fa-check">CSV</li>
										<li class="icon solid fa-check">ETL</li>
										<li class="icon solid fa-check">Visualization</li>
										<li class="icon solid fa-check">Business Analysis</li>
									</ul>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>Demo</h3>

									<h5>The process steps:</h5>
									<ol>
										<li>CSV as data extracts from different systems.</li>
										<li>Python with Pandas for ETL (Extract, Transform, Load) process.</li>
										<li>Data Warehouse (MS Access/SQL Server).</li>
										<li>Share Point lists for the data back-up.</li>
										<li>Tableau for the data visualization.</li>
										<li>Server to get dasboard published for stakeholders.</li>
										<li>Business analysis.</li>
										<li>New data coming in.</li>
										<li>Go to step 1.</li>
									</ol>

									<h4>Workflow</h4>
									<img src="images/schema.JPG" width="780">

									<h4>Python</h4>

									<h5>ETL:</h5>
									<ol>
										<li>Getting all of the flat files in the current directory and consolidating them into one Pandas dataframe.
											<br>
											<code>combined = pd.concat(dfs)</code>
										</li>
										<li>Assigning a random Region for exercies purpose:
											<br>
											<code>import random</code><br>
											<code>regions = ['AP', 'EMEA', 'AMS', 'LAC', 'NA']</code><br>
											<code>combined['Region'] = combined['Region'].map(lambda x: random.choice(regions))</code>
										</li>
										<li>Replacing values based on the condition in lambda function.
											<br>
											<code>combined['Company'] = combined['Company'].map(lambda x: 'ABC' if x in ['ZXC', 'XXX', 'YYY'] else x)</code>
										</li>
										<li>Filling up blanks based on the condition for a values in the another column.
											<br>
											<code>combined.loc[(combined['Status'] == 'Closed') & (combined['Request quality'].isnull()), 'Request quality'] = 'not evaluated'</code>
										</li>
										<li>Setting data types to specific column.
											<br>
											<code>combined['Request nr'] = combined['Request nr'].astype(str)</code>
										</li>
										<li>Saving consolidated dataframe into excel file:
											<br>
											<code>combined.to_excel('combined.xlsx', index=False)</code>
										</li>
									</ol>

									<h4>Tableau</h4>

									<h5>Sales dashboard:</h5>
									<img src="images/dashboard.JPG" width="780">

									<h5>KPI dashboard:</h5>
									<img src="images/kpi2.jpg">

									<h5>Joins, Blending, Unions</h5>
									<ul>
										<li>As we know we have two kinds of tables:
											<br>
											1. <b>Fact table</b> - a set of data containing measurements, metrics or facts of some business processes. Facts correspond to measures to Tableau.<br>
											2. <b>Dimension table</b> - a set of descriptive data that is most-likely text-based.
										</li>
										<li>We can combine data in different ways:
											<br>
											1. Joins - combining two data sets horizontally on a shared dimension.
											<br>
											- left join of two tables (dimension and fact):
											<br>
											<span style="font-size:12px;">source: OneNumber LLC</span><img src="images/lfet_join_dimension_fact.jpg">
											<br>
											- when joining we need to be careful if number of jouned records is higher than number of records in the fact table.<br>
											- as it is the left join applied, we get everything from dimensions even though there might be no fact registered for particular record - in such case we get Null assigned.
											<br>
											- full outer join on two fact tables:
											<br><br>
											<img src="images/outer_join.jpg"><span style="font-size:12px;">source: OneNumber LLC</span>
											<br>
											- as we can see there might be no sales for ice creams on Monday and no sales for chocolate on Sunday but full outer join takes all records even though there might be no matches.<br>
											2. Blending:<br>
											- always a left join.<br>
											-	we can mix the data from different sources on a common dimension.<br>
											3. Unions:<br>
											- combining data sets vertically on shared dimensions.
											<br><br>
											<img src="images/union.jpg"><span style="font-size:12px;">source: OneNumber LLC</span>
										</li>
									</ul>

									<h5>Tableau main elements:</h5>
									<ul>
										<li>There are two main fileds:
											<br>
											- <b>Dimensions</b> - categorical field.<br>
											- <b>Measures</b> - numerical fields to be aggregated.<br>
										</li>
										<li>Fileds can be presented in two ways:
											<br>
											- <b>Discrete</b> - simply headers - they slice up the data giving them labels.<br>
											- <b>Continuous</b> - they are displayed as axis - number lines with continuosus range of values without the poauses.
										</li>
									</ul>

									<h5>Calculation in Tableau:</h5>
									<ul>
										<li>Generally speaking, we can enrich data source using calculations.</li>
										<li>There are following types of calculation in Tableau:
											<br>
											- basic row level,<br>
											- basic aggreagate,<br>
											- Table calculation.<br>
											- Level Of Details.
										</li>
									</ul>

									<h5>Basic row level calculation:</h5>
									<ul>
										<li>Having below calculation:
											<br>
											<code>if YEAR([PO Sent to Supplier]) = 2019 Then [Value] End</code>
											<br>
											- we can put it into the text shelf getting:
											<img src="images/row_level_calculation.JPG">
										</li>
										<li>In the data view, we can see that those records that fulfills if condition gets [Value]. Those records that don't fulfil the condition (2019) get Nulls.</li>
									</ul>

									<h5>Basic aggregate:</h5>
									<ul>
										<li>Having below calculation:
											<br>
											<code>SUM(</code><br>
											<code>IF YEAR([PO Sent to Supplier]) = 2021</code><br>
											<code>THEN [Value]</code><br>
											<code>END</code><br>
											<code>)</code><br>
											- we aggregate values from all records that fulfills the condition into a one record value of SUM().
										</li>
									</ul>

									<h5>Table Calculation:</h5>
									<ul>
										<li>This is a kind of calculations thst is performed on aggregated measures that Tableau sees in the virtual table.</li>
										<li>We can call them as a second pass aggregation like runnin totals (accumulative sum), moving average, percent of totals and so on.</li>
										<li>In other words, calculations are made based on the result that comes back from a data source.
												Tableau issues query to the data source and returns a set of aggregated results.
												Then, Tableau proceed with table calculation inside against that set of aggregated result.
										</li>
										<li>There are some factors that affect table calculations:
											<br>
											- direction (left to right or top to bottom),<br>
											- scope (table - everythin in the view, pane - subgroups, cell - row and column intersection),<br>
											- layout (dimension in shelf - level of detail),<br>
											- filters.
										</li>
										<li>Here is the example of getting percent differene of <code>SUM(Value)</code> for a particular month:
											<br><br>
											<img src="images/percent_diff.JPG">
											<br>
											- here are the figures when we can see how percent diff works:
											<br><br>
											<img src="images/percent_diff_numbers.JPG">
											<br>
											- each row evaluates the change of current value in comparison with the previous one,<br>
											- aggregated value in Feb-19 fell from 517k to 68k with is drowdawn in - 88%,<br>
											- as we can see percent differnce is yet another calculation on aggregated measure of <code>SUM(Val)</code>.<br>
											- we can also visually recognize that table calculation is applied to a measure by the triangle icon within a pill.
										</li>
									</ul>

									<h5>LOD and LOD Expressions in Tableau:</h5>
									<ul>
										<li>Defines granuality of the data.
											<br><br>
											<img src="images/lod1.JPG"><span style="font-size:12px;">source: Tableau Software</span>
											<br>
											- in above case the level of detail of the visualization is 'region' only.<br>
											<br>
											<img src="images/lod2.JPG"><span style="font-size:12px;">source: Tableau Software</span>
											<br>
											- in above case the level of detail is more complex as we add more measures and each mark is the combination of two: 'category' and 'region'.
										</li>
										<li>Different shelves have different effect on the LOD of the view apart from dimensions themeselves.
											<br><br>
											<img src="images/lod3.JPG" width="400"><span style="font-size:12px;">source: Tableau Software</span>
											<br>
											- LOD is defined by dimensions put in the highlighted shelves.<br>
											- filter doesn't impac the LOD.
										</li>
										<li>LOD Expressions give a control over the level of granuality.
											They can be performed at a more granular level (INCLUDE), a less granular level (EXCLUDE), or an entirely independent of view level (FIXED).</li>
										<li>Granuality - each individual distinguishable piece of data.</li>
										<li>Level of Details:
											<br><br>
											<table>
												<tr>
											    <th>High Level</th>
											    <th>Low Level</th>
											  </tr>
											  <tr>
											    <td>Excludes details</td>
											    <td>Includes details</td>
											  </tr>
											  <tr>
											    <td>Summary</td>
											    <td>Detailed view</td>
											  </tr>
												<tr>
													<td>Less granular</td>
													<td>More granular</td>
												</tr>
												<tr>
													<td>More aggregated</td>
													<td>Less aggregated</td>
												</tr>
											</table>
											- We can conclude from above comparison that the higher aggregation, the lower granuality and vice versa.<br>
											- Level of detail in Tableau is what is shown on the view.
										</li>
										<li>Viz LOD:
											<br>
											- when we drag a measure to a shelf it gets aggregate to SUM in the view by default:<br>
											<br>
											<img src="images/grand_total.JPG" width="400"><span style="font-size:12px;">source: sqlbelle</span><br>
											- but we can be more precised by dragging one dimension to another shelf, receiving totals at the level of Category:<br><br>
											<img src="images/granular_total.JPG" width="400"><span style="font-size:12px;">source: sqlbelle</span><br>
											- the more dimensions we add, the more granular numbers become.<br>
										<li>Viz LOD: <code>MAX([Sales])</code>.</li>
										</li>
										<li>Custom LOD with LOD Expression:
											<br>
											- syntax:<br>
											<code>{TYPE [Dimension List]: AGGREAGATE FUNCTION}</code>,<br>
											where TYPE might be:<br>
											- FIXED: independent of view returning a scalar (a singular value,)<br>
											- EXCLUDE: minus from view,<br>
											- INCLUDE: add to view,<br>
											- and [Dimension List] is optional.<br>
										</li>
										<li>FIXED example:
											<br>
											<code>{FIXED : SUM([Sales])}</code><br><br>
											<img src="images/fixed_2.JPG" width="500"><span style="font-size:12px;">source: sqlbelle</span><br><br>
											<code>{ FIXED [Manufacturer]: MAX([Sales]) }</code><br>
											- we can read this as following: for each manufacturer count max sales.<br>
											- this computes at the <b>Manufacturer</b> level regardless of what viz LOD is.<br>
											- MAX([Sales]) will be fixed on a particular Manufacturer regardless of dimension, filters and the view overall.<br>
											<br>
											<img src="images/fixed.JPG"><span style="font-size:12px;">source: Vizual Intelligence</span><br>
										</li>
										<li>FIXED calculations are applied before dimension filters:
											<br>
											- FIXED level of detail doesn't care about what else is in the view, taking into account only passed dimension or not as it's optional.<br>
											- example: <code>SUM([Sales]) / ATTR({FIXED : SUM([Sales])})</code><br>
											- having following calculation on one shelf in a view along with [State] dimension on a different shelf,
												the calculation will give you the ratio of a state’s sales to total sales.<br>
											- if you then put [State] on the Filters shelf to hide some of the states, the filter will affect only the numerator in the calculation.
												Since the denominator is a FIXED level of detail expression,
												it will still divide the sales for the states still in the view against the total sales
												for all states—including the ones that have been filtered out of the view <span style="font-size:16px;"> (source: Tableu help)</span>.<br>
											- another example:
											<br><br>
											<img src="images/fixed_filter.JPG"><br>
											- when filtering out on [Tracker Name], the calculated total stays the same as if it got fixed:<br><br>
											<img src="images/fixed_filter2.JPG">
										</li>
										<li>Fixed Table-Scoped LOD
											<br>
											- There is no TYPE and no [Dimension List]:<br>
											<code>[Sales] - {AVG([Sales])}</code><br>
											- By calculation above we can compare store sales for each individual store to the average of sales for all stores.<br>
											- When the outcome is negative for a particular store then its sales is less than average overall sales.<br>
											- Here is the another example of comparing max year of slaes with the year of each record:<br>
											<code>IF {MAX(YEAR([Order Date]))}=YEAR([Order Date]) THEN [Sales Value]</code><br>
											<code>ELSE 0</code><br>
											<code>END</code><br>
											- when record's year doesn't equal max year from entire dataset then each record gets 0 as the result from the calculated field.<br>
											- Just to sum this up, when no FIXED, INCLUDE, EXCLUDE then { } implies FIXED on everything inside.
										</li>
										<li>EXCLUDE example:
											<br>
											<code>{EXCLUDE [Category]: SUM(Sales)}</code>
											<br><br>
											<img src="images/exclude.JPG" width="600"><span style="font-size:12px;">source: sqlbelle</span><br>
											- We want to exclude 'Category' dimension from the view and calculate the sum of Sales (like it is from the high level without any detailes provided by a dimension).<br>
											- EXCLUDE LOD is being affected by dimension filters and is getting detailed while more dimensions put in the shelves.<br>
											- The same can be done with FIXED LOD: <code>{ FIXED: SUM([Sales]) }</code><br>
											- We can also be excluding more dimensions:
											<br><br>
											<img src="images/exclude2.JPG" width="600"><span style="font-size:12px;">source: sqlbelle</span><br>
										</li>
										<li>INCLUDE LOD:
											<br>
											<code>AVG({INCLUDE [Region]: SUM(Profit)})</code>
											<br><br>
											<img src="images/include.JPG" width="600"><span style="font-size:12px;">source: sqlbelle</span><br>
											- INCLUDE puts more dimensions (more details) in the view and calculates aggregation on them.<br>
											- To the 'Category' dimension we want to include 'Region' dimension and perfom above calculation on it.
										</li>
									</ul>

									<h5>Order of operations in Tableau:</h5>
									<ul>
										<li>Here is the hierarchy of operations where we can see that FIXED LOD is not being affected by dimension filters but INCLUDE/EXCLUDE are so.
											<br>
											<img src="images/order_of_operations_overall.png" width="500"><span style="font-size:12px;">source: help.tableau.com</span><br>
										</li>
										<li>Context filter affects FIXED LOD, here is how to set it up:
											<br><br>
											<img src="images/context_filter.JPG" width="350"><span style="font-size:12px;">source: sqlbelle</span><br>
										</li>
										<li>Analogously to SQL:
											<br>
											- measure filters are equivalent to the HAVING clause in a query,<br>
											- dimension filters are equivalent to the WHERE clause.
										</li>
									</ul>

									<h5>Calculated Fields:</h5>
									<ul>
										<li>We can embded if condition withing AGGREAGATE functions.
											<br>
											<code>SUM(</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>IF [Item Quality] = "Highest Quality"</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>THEN 1</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>END</code><br>
											<code>)</code><br>
											<code>/</code><br>
											<code>SUM(</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>IF [Item Quality] <> ""</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>THEN 1</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>END</code><br>
											<code>)</code><br>
										</li>
										<li>Every record that contains "Highest Quality" gets 1 in numerator and 1 in denominator. One divided by one gives one which we can see in the <b>Perfect to All</b> field for each record.
											<br>
											<img src="images/raw_data_with_calculated_field.JPG" width="700">
											<br>
											Every record that contains something different than "Highest Quality" gets Null in numeratior and 1 in denominator. Null divided by one gives Null.
											Overall outcome is the sum of all ones by number of records affected which gives us average percentage of "Highest Quality" to all records.
											<br>
											<img src="images/raw_data_with_calculated_field_2.JPG" width="700">
										</li>
										<li>We can then put the calculated field into a shelf to get ratio in the view:
											<br><br>
											<img src="images/calculated_ratio.jpg">
										</li>
										<li>We can be embedding ifs:
											<br><br>
											<img src="images/embeded_ifs.JPG">
										</li>
									</ul>

									<h5>Parameters:</h5>
									<ul>
										<li>Parameter is a variable that can be used across the workbook.</li>
										<li>It is a field that accepts input from the end user who takes control over the visualization.</li>
										<li>Here are some cases when the parameter can be used:
											<br>
											- numeric thresholds,<br>
											- what-if analysis,<br>
											- dynamic field, axis, titles,<br>
											- filtering across different data sources,<br>
											- top N.
										</li>
										<li>How to set it up:
											<br>
											1. Creating parameter.<br>
											2. Placing parameter in either a calculated field, reference line, set or filter.<br>
											3. Putting an input field to the view.
										</li>
										<li>For example I can be asking user about 'Value Threshold'. Then I need to create a parameter that accepts integer, showing the input field:
											<br><br>
											<img src="images/parameter.JPG">
											<br>
											We can put the variable in the calculated field:
											<br>
											<code>IF SUM([Value]) >= [Value Threshold]</code><br>
											<code>THEN "OK"</code><br>
											<code>END</code>
											<br>
											Having set calculated field, we can drag and drop it into color shelf:
											<br><br>
											<img src="images/parameter2.JPG" width="700">
											<br>
											- as we can see, there is the color distinction for values > 1,000,000.00
										</li>
										<li>And here is another example of a dynamic dimension:
											<br>
											- parameter (Select Dimension):
											<br><br>
											<img src="images/parameter3.JPG">
											<br>
											- calculated field (Dynamic Dimension):<br>
											<code>CASE [Select Dimension]</code><br>
											<code>WHEN 'Company' THEN [Group Company]</code><br>
											<code>WHEN 'Region' THEN [Region]</code><br>
											<code>WHEN 'Tracker' THEN [Tracker Name]</code><br>
											<code>END</code><br>
											- each selection of the parameter in [Comapny, Region, Tracker] will reflect with different dimension on Rows shelf (due to calculated field):
											<br><br>
											<img src="images/parameter4.JPG" width="500">
										</li>
									</ul>

									<h5>Color shlef:</h5>
									<ul>
										<li>We can use claculated field in order put it into the color shelf:
											<br>
											<code>IF AVG([Discount]) > 0.05 THEN 'High'</code><br>
											<code>ELSE 'Low'</code><br>
											<code>END</code>
										</li>
										<li>It differentiates continues measures:
											<br><br>
											<img src="images/calculated_field_on_color_shelf.JPG" width="600"><span style="font-size:12px;">source: Jeffrey Shaffer</span>
										</li>
									</ul>

									<h5>Row Level Security:</h5>
									<ul>
										<li>A kind of the user filter that restricts access at the data row level.</li>
										<li>In the filter we have to specify which row can be visible for any given person on the server.</li>
										<li>Example usecase:
											<br>
											<i>When publishing the report, you want to allow each regional manager to see only the data relevant to his or her region.
												Rather than creating a separate view for each manager, you can apply a user filter that restricts access to the data based on users’ characteristics, such as their role.</i>
										</li>
										<li>In order to apply RLS we can create a dynamic filter using a calculated field that automates the process of mapping users to data rows.
												This method requires that the underlying data includes the security information for filtering.
												For example, if you want to filter a view so that only supervisors can see it, the underlying data must be set up to include user names and specify each user’s role.
										</li>
										<li>For example we have some regions and we want to assign a supervisor to each of them:
											<br><br>
											<img src="images/rsl.JPG">
											<br>
											That way we keep security information within the dataset.
											<br><br>
											<img src="images/rsl2.JPG">
											<br>
											In order to get the data filtered by the current user (supervisor) who actually views the analysis we need to create the calcualted field making usage of a so-called user function:
											<br><br>
											<img src="images/rsl3.JPG">
											<br>
											so that we can put it into the filters shelf having only TRUE selected. Here is the final look of the view:
											<br><br>
											<img src="images/rsl4.JPG">
											<br>
											In fact, we can get rid of the <b>Supervisor</b> pill from rows shelf.
											<br><br>
											<img src="images/rsl5.JPG">
											<br>
											That way, when publishing the analysis only relevant ro superviosr regions will be displayed without having to create different views for each reagion (supervisor).
										</li>
									</ul>

									<h5>Sets</h5>
									<ul>
										<li>Custom fields being a subset of a specific field that we can create based on certain condition.</li>
										<li>Example:
											<br>
											- let's say we want to visually separate top 3 regions with the highest EUR spend:
											<br><br>
											<img src="images/set1.JPG">
											<br>
											turning the top option on other numeric field:
											<br><br>
											<img src="images/set2.JPG">
											<br>
											newly-created set in placed in the set section on side bar from which I can drag and drop on color shelf making top 3 regions distinct:
											<br><br>
											<img src="images/set3.JPG">
										</li>
									</ul>

									<h5>Bins</h5>
									<ul>
										<li>Creating bins helps us to cluster measures into customizable size of ranges.</li>
										<li>Getting bins on a measure, we convert numerical variable (measure) into categorical variable (dimension).</li>
										<li>Perfect solution if numerical data is a type of float or double.</li>
										<li>Here is the perfect example when we want to change this distribution of ages:
											<br><br>
											<img src="images/bins1.JPG" width="700"><span style="font-size:12px;">source: Art of Visualization</span>
											<br>
											and to get the Age bins in the size of 5:
											<br><br>
											<img src="images/bins2.JPG" width="550"><span style="font-size:12px;">source: Art of Visualization</span>
											<br>
											- in the example above 15 is inclusive, and 20 is not etc.
										</li>
									</ul>

									<h5>Write Back Extension:</h5>
									<ul>
										<li>The functionality that allows to input data through Tableau and store it in the data source for a future reference.</li>
										<li>We can go with 2 versions:
											<br>
											- free: we can save the data into a google spreadsheet,<br>
											- commercial: we can save the data into a database.
										</li>
										<li>The extension works on dashboard level which has at least one sheet laid.
												It can be dragged from the objects section and then chosen from the extensions gallery.
												Once downloaded we can load it up to the dashboard picking it up from the browsing window.
												Lastly, configuration window shows up in order to proceed with the extension's details.
										</li>
									</ul>


									<h5>Conclusions:</h5>
									<ul>
										<li>Python script combines flat files into one Tableau input.</li>
										<li>In this case I use static source which is CSV flat files (batch processing).</li>
										<li>However data pipelines are built also for real-time sources where target system requires constant data update (stream processing).</li>
										<li>Data pipeline can route the processed data into another applications like Tableu for building dashboards.</li>
										<li>Once dashboard built, there is the Tableau option for sending it into server.</li>
										<li>Once published, we can perfrom Business Analysis.</li>
										<li>Making good business decisions based on BA leads to process improvements.</li>
										<li>Process improvements generates new data that can undergo the Data Warehousing all over again.</li>
										<li>Data Warehouse is a great concept for monitoring business processes and continuous improvement implementation.</li>
										<li>Taking conclusions from the analysis that can improve business perfomance.</li>
									</ul>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>Setup</h3>
									<h5>Following installation required:</h5>
									<code>pip install pandas</code><br>
									<code>Tableau</code>
								</div>
							</section>

						<!-- Five -->
							<section id="five">
								<div class="container">
									<h3>Source Code</h3>
									<p>You can view the source code: <a href="https://github.com/ArturSkrzeta/Data-Warehouse-CSV-Python-Tableau-Server-BA-New-Data/tree/master/project">HERE</a></p>
									<p>&nbsp</p>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<a href="https://arturskrzeta.github.io/" style="padding-bottom:10px;">Back to Portfolio</a>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
